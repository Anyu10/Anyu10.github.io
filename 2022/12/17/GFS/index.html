<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"anyu10.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.14.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="IntroductionBig storage abstraction is important in distributed systems. Why is it hard?  Performance –&gt; Sharding Faults –&gt; Tolerance Tolerance –&gt; Replication Replication –&gt; Consistency Co">
<meta property="og:type" content="article">
<meta property="og:title" content="GFS">
<meta property="og:url" content="http://anyu10.github.io/2022/12/17/GFS/index.html">
<meta property="og:site_name" content="Anyu">
<meta property="og:description" content="IntroductionBig storage abstraction is important in distributed systems. Why is it hard?  Performance –&gt; Sharding Faults –&gt; Tolerance Tolerance –&gt; Replication Replication –&gt; Consistency Co">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="d:\Blog\source_images\repl.drawio.png">
<meta property="og:image" content="d:\Blog\source_images\image-20221221170828501.png">
<meta property="og:image" content="d:\Blog\source_images\image-20221221223610132.png">
<meta property="article:published_time" content="2022-12-17T10:54:23.000Z">
<meta property="article:modified_time" content="2022-12-22T13:58:34.450Z">
<meta property="article:author" content="Anyu Elin">
<meta property="article:tag" content="Distributed System">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="d:\Blog\source_images\repl.drawio.png">


<link rel="canonical" href="http://anyu10.github.io/2022/12/17/GFS/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://anyu10.github.io/2022/12/17/GFS/","path":"2022/12/17/GFS/","title":"GFS"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>GFS | Anyu</title>
  






  <script async defer data-website-id="" src=""></script>

  <script defer data-domain="" src=""></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Anyu</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Design"><span class="nav-number">2.</span> <span class="nav-text">Design</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Assumption"><span class="nav-number">2.1.</span> <span class="nav-text">Assumption</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Interface"><span class="nav-number">2.2.</span> <span class="nav-text">Interface</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture"><span class="nav-number">2.3.</span> <span class="nav-text">Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Overview"><span class="nav-number">2.3.1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-Master"><span class="nav-number">2.3.2.</span> <span class="nav-text">Single Master</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Chunk-Size"><span class="nav-number">2.3.3.</span> <span class="nav-text">Chunk Size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Metadata"><span class="nav-number">2.3.4.</span> <span class="nav-text">Metadata</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consistency-Model"><span class="nav-number">2.3.5.</span> <span class="nav-text">Consistency Model</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Implementation"><span class="nav-number">3.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lease-and-Mutation-Order"><span class="nav-number">3.1.</span> <span class="nav-text">Lease and Mutation Order</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Flow"><span class="nav-number">3.2.</span> <span class="nav-text">Data Flow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Atomic-Append"><span class="nav-number">3.3.</span> <span class="nav-text">Atomic Append</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Snapshot"><span class="nav-number">3.4.</span> <span class="nav-text">Snapshot</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Master-Operation"><span class="nav-number">4.</span> <span class="nav-text">Master Operation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Namespace-Management-and-Locking"><span class="nav-number">4.1.</span> <span class="nav-text">Namespace Management and Locking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Replica-Placement-Physically"><span class="nav-number">4.2.</span> <span class="nav-text">Replica Placement (Physically)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Replicas-Management"><span class="nav-number">4.3.</span> <span class="nav-text">Replicas Management</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Garbage-Collection"><span class="nav-number">4.4.</span> <span class="nav-text">Garbage Collection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stale-Replica-Detection"><span class="nav-number">4.5.</span> <span class="nav-text">Stale Replica Detection</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fault-Tolerance"><span class="nav-number">5.</span> <span class="nav-text">Fault Tolerance</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Availability"><span class="nav-number">5.1.</span> <span class="nav-text">Availability</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Integrity"><span class="nav-number">5.2.</span> <span class="nav-text">Data Integrity</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">6.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Anyu Elin"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Anyu Elin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://anyu10.github.io/2022/12/17/GFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Anyu Elin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Anyu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="GFS | Anyu">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GFS
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-17 18:54:23" itemprop="dateCreated datePublished" datetime="2022-12-17T18:54:23+08:00">2022-12-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-12-22 21:58:34" itemprop="dateModified" datetime="2022-12-22T21:58:34+08:00">2022-12-22</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>3.2k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>11 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Big storage abstraction is important in distributed systems.</p>
<p>Why is it hard?</p>
<ul>
<li>Performance –&gt; Sharding</li>
<li>Faults –&gt; Tolerance</li>
<li>Tolerance –&gt; Replication</li>
<li>Replication –&gt; Consistency</li>
<li>Consistency –&gt; Low Performance</li>
</ul>
<span id="more"></span>

<p>Why we need GFS?</p>
<ul>
<li>A strawman replication design:</li>
</ul>
<p><img src="D:\Blog\source_images\repl.drawio.png" alt="repl.drawio"></p>
<ul>
<li>User1 write 1 to two machines, User2 write 2 to two machines</li>
<li>Due to the network, server1 see 2 first; server2 see 1 first.</li>
<li>Consistency is destroyed!</li>
</ul>
<p>How about GFS?</p>
<ul>
<li>Performance: Sharding, every file will be automatically sharded by GFS.</li>
<li>Performance: it has high aggregate performance to a large number of clients. </li>
<li>Fault Tolerance: Automatic failure recovery.</li>
<li>Designed for a single datacenter.</li>
<li>Designed for big sequential access.</li>
<li>Designed for data-intensive applications.</li>
</ul>
<p>What problem does GFS solve?</p>
<ul>
<li>Component failures are norm rather than exception<ul>
<li>Constant monitoring, error detection, fault tolerance, and automatic recovery are needed.</li>
</ul>
</li>
<li>Files are huge, so it’s unwieldy to manage billions of KB-sized files.<ul>
<li>Design choices of I&#x2F;O operation and block sizes have to be revisited.</li>
</ul>
</li>
<li>Appending and sequential reading are more common than write operation.<ul>
<li>Appending become the focus of optimization and atomicity guarantee instead of caching data blocks in the client (like what normal filesystems do).</li>
</ul>
</li>
<li>Co-designing the applications and the file system API, relaxing GFS’s consistency model, and designing an atomic append operation.</li>
</ul>
<h1 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h1><h2 id="Assumption"><a href="#Assumption" class="headerlink" title="Assumption"></a>Assumption</h2><ul>
<li>System is built from inexpensive components that <em>usually fails</em> –&gt; so it needs monitor and detect, tolerate and recovery from component failure.</li>
<li>System stores mostly large files. <em>Large files operations</em> should be efficient.</li>
<li>Two kinds of reads: <strong>large streaming read</strong> and <strong>small random reads</strong>.<ul>
<li>Performance-conscious applications often batch and sort small reads to advance steadily through the file rather than go back and forth.</li>
</ul>
</li>
<li>Two kinds of writes: <strong>large, sequential writes that append data to files</strong> (normal and need to be optimized) and <strong>small writes at random positions</strong> (rare and need not to be efficient).</li>
<li>Should support well-defined semantic of <em>appending files concurrently</em> –&gt; Atomicity with minimal synchronization overhead is needed.</li>
<li><strong>High sustained bandwidth</strong> is more important than low latency.</li>
</ul>
<h2 id="Interface"><a href="#Interface" class="headerlink" title="Interface"></a>Interface</h2><p>Files are <em>organized hierarchically</em> in directories and <em>identified by pathnames</em>.</p>
<p>support usual operations like <em>create, delete, open, close, read, write</em>.</p>
<p>GFS also has <em>snapshot</em> and <em>record append</em> operations</p>
<ul>
<li><strong>Snapshot</strong> creates a copy of a file or a directory tree at low cost. </li>
<li><strong>Record append</strong> –&gt; atomic append, allows multiple clients to append data to the same file concurrently.</li>
<li>Useful for implementing <strong>multi-way merging</strong> and <strong>producer-consumer queues</strong>.</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><img src="D:\Blog\source_images\image-20221221170828501.png" alt="image-20221221170828501"></p>
<p>GFS Clusters:</p>
<ul>
<li><p>single <em>master</em></p>
</li>
<li><p>multiple <em>chunkserver</em></p>
</li>
<li><p>multiple client (client and chunkserver can be run on same machine)</p>
</li>
<li><p>Each of these is typically a commodity Linux machine running a user-level server process.</p>
</li>
</ul>
<p>File Storage:</p>
<ul>
<li>Files are divided into <strong>fix-sized</strong> chunks.</li>
<li>Each <strong>chunks</strong> identified by an immutable and globally unique 64 bit chunk handle.</li>
<li><strong>Chunk handle</strong> is assigned by the master at the time of chunk creation.</li>
<li>Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range.</li>
<li>Each chunk is replicated on multiple chunkservers.</li>
</ul>
<p>Single Master:</p>
<ul>
<li>Maintains all filesystem metadata.<ul>
<li>Namespace.</li>
<li>Access control information.</li>
<li>Mapping from file to chunks.</li>
<li>Current location of chunks.</li>
</ul>
</li>
<li>Control system-wide activities.<ul>
<li>Chunk lease management.</li>
<li>Garbage collection of orphaned chunks.</li>
<li>Chunk migration between chunkservers.</li>
</ul>
</li>
<li>Periodically communicate with chunkservers in <em>HeartBeat</em> messages to <em>give it instructions</em> and <em>collect its state</em>.</li>
</ul>
<p>Client:</p>
<ul>
<li>implement the filesystem API.</li>
<li>Communicate with master for metadata and chunkserver for data.</li>
<li>Neither client nor chunkserver cache the data.<ul>
<li>For Client, most app stream through the file, it’s too large to be cached.</li>
<li>it also eliminate cache coherence issues. (<strong>client cache metadata instead</strong>)</li>
<li>For Chunkserver, Linux’s buffer cache is enough.</li>
</ul>
</li>
</ul>
<h3 id="Single-Master"><a href="#Single-Master" class="headerlink" title="Single Master"></a>Single Master</h3><p>Benefit: simplify the design and enable master to make sophisticated chunk replacement and replication decisions by using global knowledge.</p>
<p>But we need to minimize the read and write on master so that it doesn’t become the bottleneck.</p>
<p>So Client only ask master for metadata, and caches this information for a limited time and interacts with the chunkservers directly for many subsequent operations.</p>
<hr>
<p>Simple read operation:</p>
<ol>
<li>Client translates file name and byte offset into chunk index within the file.</li>
<li>Client sends request to master containing chunk index and file name.</li>
<li>Master replies with the corresponding chunk handle and location of replicas.</li>
<li>Client caches the information using the fila name and chunk index as the key.</li>
<li>Client then sends the request to one of the replicas (mostly the closest one), the request specifies the chunk range and chunk handle<ul>
<li>In fact, the client typically asks for multiple chunks in the same request and the master can also include the informa- tion for chunks immediately following those requested</li>
</ul>
</li>
</ol>
<h3 id="Chunk-Size"><a href="#Chunk-Size" class="headerlink" title="Chunk Size"></a>Chunk Size</h3><p>Chunk Size, 64MB, is much larger than file system block sizes.</p>
<p>Each chunk is stored as plain Linux file and extended only as needed. This kind of lazy allocation avoids space wasting due to internal fragmentation.</p>
<p>Advantages of large chunk size:</p>
<ul>
<li>Reduce client’s needs to interact with master.</li>
<li>Since on a large chunk, a client is more likely to perform many operations on a given chunk.</li>
<li>Reduces the size of the metadata stored on the master. Thus we can keep the metadata in memory.</li>
</ul>
<p>Disadvantages of large chunk size:</p>
<ul>
<li>Small files consist a small number of chunks. The chunkservers storing those chunks may become hot spots if many clients are accessing the same file</li>
<li>In practice, hot spot is not a major problem. But hot spots did develop when GFS was first used by a batch-queue system.<ul>
<li>One solution: making the batch-queue system stagger application start times</li>
<li>A potential long-term solution: allow clients to read data from other clients in such situations.</li>
</ul>
</li>
</ul>
<h3 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h3><p>All metadata is kept in memory, there are three kind of metadata:</p>
<ol>
<li>The file and chunk namespace.</li>
<li>The mapping from files to chunks.</li>
<li>location of each chunks’ replicas.</li>
</ol>
<p>First two kinds of metadata (namespaces and file-to-chunk mapping) is logged on local disk and replicated by a remote machine. The master doesn’t store chunk location information, instead, it ask each chunkserver about its chunks at master’s startup and whenever chunkserver joins the cluster.</p>
<hr>
<p>Why we want data structure in-memory?</p>
<ul>
<li>Since metadata is stored in memory, master can easily periodically scan the entire state <strong>in the background</strong>. This periodic scanning is used to implement <strong>chunk garbage collecting</strong>, <strong>re-replication in the presence of chunkserver failures</strong>, <strong>chunk migration between chunkservers for balance-loading and disk space usage</strong>.</li>
</ul>
<p>Why we don’t need to keep chunk location persistent in disk?</p>
<ul>
<li>The master can keep chunk location up-to-date thereafter because it controls all chunk placement and monitors chunkserver status <strong>with regular HeartBeat messages</strong>. This also eliminate the problem of <em>keeping the master and chunkservers in sync</em> as chunkservers <em>join and leave the cluster, change names, fail, restart, and so on</em>.</li>
</ul>
<p>What is operation log?</p>
<ul>
<li>It contains historical record of critical metadata changes.<ul>
<li>It is the only persistent record of metadata.</li>
<li>It serves as local timeline that defines the order of concurrent operations.</li>
<li>Files, chunks and their versions are identified by local time.</li>
</ul>
</li>
<li>Operation log must be stored reliably and we should not make changes visible to clients until metadata changes are made persistent.</li>
<li>We replicate it on multiple remote machines and respond to a client operation only after flushing the corresponding log record to disk both locally and remotely.</li>
<li>The master batches several log records together before flushing thereby reducing the impact of flushing and replication on overall system throughput.</li>
</ul>
<h3 id="Consistency-Model"><a href="#Consistency-Model" class="headerlink" title="Consistency Model"></a>Consistency Model</h3><p>GFS has a relaxed consistency model</p>
<p>What is guarantees by GFS?</p>
<ul>
<li><p><strong>File namespaces mutations are atomic</strong>, it’s handled by master using a namespace locking. And the global total order is defined by master’s operation log file.</p>
</li>
<li><p>File region state after mutation</p>
<ul>
<li>A file region is consistent if all clients will always see the same data, regardless of which replicas they read from.</li>
<li>A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety. </li>
<li>When a mutation succeeds without interference from concurrent writers, the affected region is defined.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>write</th>
<th>record append</th>
</tr>
</thead>
<tbody><tr>
<td>Serial success</td>
<td>defined</td>
<td>defined, interspersed with inconsistent</td>
</tr>
<tr>
<td>Concurrent success</td>
<td>consistent but undefined</td>
<td>defined, interspersed with inconsistent</td>
</tr>
<tr>
<td>Failure</td>
<td>inconsistent</td>
<td>inconsistent</td>
</tr>
</tbody></table>
</li>
<li><p>Atomic append –&gt; data appended <em>at least once</em>, but at an offset of GFS’s choosing.</p>
<ul>
<li>The offset is returned to the client and marks the beginning of a defined region that contains the record.</li>
<li>GFS may insert padding or record duplicates in between.</li>
</ul>
</li>
<li><p>After a sequence of successful mutations, the mutated file region is guaranteed to be defined.</p>
<ul>
<li>By applying mutations to a chunk in same order on all replicas.</li>
<li>By using chunk version to detect stale replica.</li>
<li>Stale chunk will be garbage collected.</li>
</ul>
</li>
<li><p>Client cache the chunk locations, they may read from stale replicas.</p>
<ul>
<li>This window is limited by lease time</li>
<li>stale replica usually return premature end rather than outdated data since most files are append-only.</li>
</ul>
</li>
</ul>
<p>Implication of Applications</p>
<ul>
<li>append-only rather than writes</li>
<li>checkpointing</li>
<li>self-validating, e.g. record checksums</li>
<li>self-identifying, e.g. record identifier</li>
</ul>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><h2 id="Lease-and-Mutation-Order"><a href="#Lease-and-Mutation-Order" class="headerlink" title="Lease and Mutation Order"></a>Lease and Mutation Order</h2><p>We use lease to maintain a consistent mutation order across replicas. The master grant a chunk lease to one of the replicas, called <em>primary</em>. A <em>primary</em> picks a mutation ordering for all replicas.</p>
<p>Lease mechanism is designed to <strong>minimize management overhead of master</strong>. It has timeout of 60s. </p>
<p><img src="D:\Blog\source_images\image-20221221223610132.png" alt="image-20221221223610132"></p>
<p>Write operation:</p>
<ol>
<li>Client asks Master who is the primary and other replica locations. If there is no primary, master then  grants one</li>
<li>Master replies the information, and client cache the data for future mutations. It needs to request master again only when primary becomes unreachable.</li>
<li>Client pushes data to all replicas. Chunkserver stores the data in local LRU buffer cache until the data is used or aged out.</li>
<li>Once the replicas have acknowledged the data, Client sends write request to primary. The primary assigns consecutive serial numbers to mutations.</li>
<li>The primary forward the write request to all replicas.</li>
<li>The secondary reply to primary when completed.</li>
<li>The primary replies to the client. <ul>
<li>If error happens at the secondary, primary sends error message to client; Then the client request is considered failed, and the modified region is left in an <strong>inconsistent state</strong>.</li>
<li>If it had failed at the primary, it would not have been assigned a serial number and forwarded.</li>
<li>Client code handles such errors by retrying the failed mutation.</li>
</ul>
</li>
</ol>
<p>If a write by the application is large or straddles a chunk boundary, GFS client code breaks it down into multiple write operations. Final result is consistent but maybe undefined because of interleave of other write operations.</p>
<h2 id="Data-Flow"><a href="#Data-Flow" class="headerlink" title="Data Flow"></a>Data Flow</h2><p>The control flow and data flow are decoupled, data is <strong>pushed linearly</strong> along a <strong>carefully picked chain</strong> of chunkservers <strong>in a pipelined fashion</strong> to fully <strong>utilize network bandwidth</strong> (so full outbound bandwidth is used to transfer data as fast as possible).</p>
<p>To <strong>avoid high-latency links</strong>, each machine forward data to “closest” machine in network that has not received the data.</p>
<h2 id="Atomic-Append"><a href="#Atomic-Append" class="headerlink" title="Atomic Append"></a>Atomic Append</h2><p>Difference between traditional write and record append?</p>
<ul>
<li>Traditional write: Client specify data and offset –&gt; the concurrent write is not serializable.</li>
<li>In record append (atomic append), Client only specify the data, GFS determine the offset and appends data to the file at least once atomically. And GFS returns the offset to the client.</li>
</ul>
<p>Why we need atomic append?</p>
<ul>
<li>Without atomic append, Clients would need more complicated and expensive synchronization, like through a lock manager.</li>
</ul>
<p>How does atomic append do?</p>
<ul>
<li><p>Record append is mutation that follows the control flow above with only a little extra logic at the primary. </p>
<ul>
<li><p>The primary checks to see if appending the record to the current chunk would cause the chunk to exceed the maximum size (64 MB). If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client indicating that the operation should be retried on the next chunk.</p>
</li>
<li><p>Record append is restricted to be at most one-fourth of the maximum chunk size to keep worstcase fragmentation at an acceptable level.</p>
</li>
</ul>
</li>
</ul>
<p>How to deal with the inconsistency caused by atomic append?</p>
<ul>
<li>If a record append fails at any replica, the client retries the operation. </li>
<li>As a result, replicas of the same chunk may contain different data possibly including duplicates of the same record in whole or in part. GFS does not guarantee that all replicas are bytewise identical.</li>
<li>Record is written <em><strong>at least once</strong></em>!</li>
<li>Application is responsible for dealing with the inconsistency.</li>
</ul>
<h2 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h2><p>How to implement snapshot using a <strong>copy-on-write</strong> technique?</p>
<ol>
<li>Master receive a snapshot request and then revoke leases.</li>
<li>Master log the request to disk. Then create new files but pointing to same chunks as source files.</li>
<li>When Client want to write to chunk C in new files, It sends request to master to find lease holder, The Master noticed that the reference to C is greater than 1, It then asks each chunkserver that has a current replica of C to create a new chunk called C’. <ul>
<li>By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network.</li>
</ul>
</li>
</ol>
<h1 id="Master-Operation"><a href="#Master-Operation" class="headerlink" title="Master Operation"></a>Master Operation</h1><p>Master manage namespace operations, manage chunk replicas throughout the system, make placement decisions, create new chunks, coordinate system-wide activities, balance load.</p>
<h2 id="Namespace-Management-and-Locking"><a href="#Namespace-Management-and-Locking" class="headerlink" title="Namespace Management and Locking"></a>Namespace Management and Locking</h2><p>Unlike traditional filesystems, GFS logically represent its namespace as lookup table mapping pathname to metadata. </p>
<p>Prefix compression –&gt; makes it suitable in memory. </p>
<p>Each node in lookup table has a read-write lock. </p>
<p>How to lock?</p>
<ul>
<li>Since the namespace can have many nodes, read-write lock objects are allocated lazily and deleted once they are not in use.</li>
<li>locks are acquired in a consistent total order to prevent deadlock: they are first ordered by level in the namespace tree and lexicographically within the same level.</li>
</ul>
<h2 id="Replica-Placement-Physically"><a href="#Replica-Placement-Physically" class="headerlink" title="Replica Placement (Physically)"></a>Replica Placement (Physically)</h2><p>Purpose of chunk replica placement policy:</p>
<ul>
<li>maximize data reliability and availability</li>
<li>maximize network bandwidth utilization</li>
</ul>
<p>Method: Spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or offline (such as network partitioning).</p>
<h2 id="Replicas-Management"><a href="#Replicas-Management" class="headerlink" title="Replicas Management"></a>Replicas Management</h2><p>Chunk replicas are created for three reasons: </p>
<ul>
<li>chunk creation</li>
<li>re-replication</li>
<li>rebalancing.</li>
</ul>
<p>Where to place initial empty replicas when creating a chunk?</p>
<ul>
<li>We want to place new replicas on chunkserver whose spare disk space is more than average.</li>
<li>We want to limit recent creation on chunkserver (to avoid heavy write on this chunkserver)</li>
<li>We want to spread chunk replicas across racks.</li>
</ul>
<p>The master re-replicates a chunk as soon as the number of available replicas falls below a user-specified goal. Similar to creating a chunk replica.</p>
<p>The master rebalances replicas periodically: it examines the current replica distribution and moves replicas for better disk space and load balancing.</p>
<h2 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a>Garbage Collection</h2><p>After a file is deleted, GFS does not immediately reclaim the available physical storage. It does so only lazily during regular garbage collection at both the file and chunk levels</p>
<p>How GFS do garbage collection?</p>
<ul>
<li><p>For Master</p>
<ul>
<li><p>When a file is deleted, it is logged by master immediately. But master rename it to a hidden name with  deletion timestamp.</p>
</li>
<li><p>During the master’s regular scan of the file system namespace, it removes any such hidden files if they have existed for more than three days.</p>
</li>
</ul>
</li>
<li><p>For Chunkserver</p>
<ul>
<li>In a similar regular scan of the chunk namespace, the master identifies orphaned chunks and erases the metadata for those chunks. </li>
<li>In a HeartBeat message regularly exchanged with the master, each chunkserver reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata. </li>
<li>Then the chunkserver is free to delete its replicas of such chunks.</li>
</ul>
</li>
</ul>
<p>Benefit:</p>
<ol>
<li>Simple and reliable for distributed system.</li>
<li>It merge storage reclamation into regular background activities.</li>
<li>The delay in reclaiming storage provides a safety net against accidental, irreversible deletion.</li>
</ol>
<p>Main disadvantage: frequently creation and deletion will cause the space tight.</p>
<p>Solution: expediting storage reclamation if a deleted file is explicitly deleted again.</p>
<h2 id="Stale-Replica-Detection"><a href="#Stale-Replica-Detection" class="headerlink" title="Stale Replica Detection"></a>Stale Replica Detection</h2><p>Master increase the chunk version number when it grant a new lease on a chunk. When a replica is not available at this time, its chunk version will not advanced. So when it restart and report its chunk version number to Master, Master will detect that it is a stale replica. </p>
<p>The master removes stale replicas in its regular garbage collection </p>
<h1 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h1><p>Two problems caused by <strong>component failure</strong>:</p>
<ul>
<li>unavailability</li>
<li>corrupted data</li>
</ul>
<h2 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a>Availability</h2><p>Two strategies for high availability:</p>
<ul>
<li>Fast Recovery<ul>
<li>Restore state and restart no matter how they terminated.</li>
</ul>
</li>
<li>Replication<ul>
<li>Chunk Replication</li>
<li>Master Replication</li>
</ul>
</li>
</ul>
<h2 id="Data-Integrity"><a href="#Data-Integrity" class="headerlink" title="Data Integrity"></a>Data Integrity</h2><p>Why we need checksum in chunkserver to detect corruption of stored data?</p>
<ul>
<li>Disks and machines failures are common, causing data corruption.</li>
<li>But it’s impractical to compare replicas to detect corruption. Besides, the semantic of record append doesn’t guarantee identical replicas.</li>
<li>Therefore, each chunkserver must independently verify the integrity of its own copy by maintaining checksums.</li>
</ul>
<p>What happens when reading corrupted data?</p>
<ul>
<li>For Reads, chunkserver verify checksum of data block before returning any data to requester.</li>
<li>If it doesn’t match the checksum, it returns a error to requester and report a mismatch to the master.</li>
<li>Master re-replicate the chunk.</li>
<li>Then master instruct deletion of the corrupted replica.</li>
</ul>
<p>How to calculate checksum when writing data?</p>
<ul>
<li>For appending, There is no need to read the data to checksum first. Because we can checksum later when reading this data.</li>
<li>For random writing, we must read the data and checksum first.</li>
</ul>
<p>Like other metadata, checksums are kept in memory and stored persistently with logging, separate from user data.</p>
<p>During idle periods, chunkservers can scan and verify the contents of inactive chunks.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>GFS structure:</p>
<ul>
<li>Master Data: two tables that matter<ul>
<li>file name –&gt; array of chunk handles</li>
<li>handle –&gt; list of chunk servers, version number, primary, lease expiration</li>
<li>Log and Checkpoint of the 2 above tables –&gt; disk</li>
</ul>
</li>
</ul>
<p>GFS read operation:</p>
<ol>
<li>Client sends file and offset to Master.</li>
<li>Master sends back chunk handles and list of servers back to Client.<ul>
<li>Client cache the map.</li>
</ul>
</li>
<li>Client talk to one of the chunk servers (maybe one closest to the Client)<ul>
<li>chunk server runs Linux filesystem, chunk is stored by file whose name is chunk handle.</li>
</ul>
</li>
<li>Chunk server sends back the chunk files to Client.</li>
</ol>
<p>GFS write(append) operation:</p>
<ul>
<li><p>No Primary</p>
<ol>
<li>Find up-to-date replicas (according to the version numbers in the master).</li>
<li>Pick a primary and a secondary.</li>
<li>Increase the version number.</li>
<li>Sends the primary and secondary with the updated version number and messages and <em>lease</em>.</li>
<li>Master write the version number to the disk.</li>
</ol>
</li>
<li><p>Primary</p>
<ol>
<li>Client sends message to Primary.</li>
<li>Primary sends message to Secondary.</li>
<li>If all Secondary replies ok, Primary sends ok to Client.</li>
<li>Else Primary sends not ok to Client.</li>
</ol>
</li>
</ul>
<p>why we need lease?</p>
<ul>
<li>to avoid “split brain” problem caused by network partition</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Distributed-System/" rel="tag"><i class="fa fa-tag"></i> Distributed System</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/12/16/Go-Network/" rel="prev" title="Go Network">
                  <i class="fa fa-chevron-left"></i> Go Network
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/19/Go-Profile/" rel="next" title="Go-Profile">
                  Go-Profile <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Anyu Elin</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">20k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">1:12</span>
  </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
